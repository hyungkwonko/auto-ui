{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Env setting\n",
    "from __future__ import print_function, division, unicode_literals\n",
    "# import example_helper\n",
    "import json\n",
    "import csv\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import emoji\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_emojis\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "# Emoji map in emoji_overview.png\n",
    "EMOJIS = \":joy: :unamused: :weary: :sob: :heart_eyes: \\\n",
    ":pensive: :ok_hand: :blush: :heart: :smirk: \\\n",
    ":grin: :notes: :flushed: :100: :sleeping: \\\n",
    ":relieved: :relaxed: :raised_hands: :two_hearts: :expressionless: \\\n",
    ":sweat_smile: :pray: :confused: :kissing_heart: :heartbeat: \\\n",
    ":neutral_face: :information_desk_person: :disappointed: :see_no_evil: :tired_face: \\\n",
    ":v: :sunglasses: :rage: :thumbsup: :cry: \\\n",
    ":sleepy: :yum: :triumph: :hand: :mask: \\\n",
    ":clap: :eyes: :gun: :persevere: :smiling_imp: \\\n",
    ":sweat: :broken_heart: :yellow_heart: :musical_note: :speak_no_evil: \\\n",
    ":wink: :skull: :confounded: :smile: :stuck_out_tongue_winking_eye: \\\n",
    ":angry: :no_good: :muscle: :facepunch: :purple_heart: \\\n",
    ":sparkling_heart: :blue_heart: :grimacing: :sparkles:\".split(' ')\n",
    "\n",
    "def top_elements(array, k):\n",
    "    ind = np.argpartition(array, -k)[-k:]\n",
    "    return ind[np.argsort(array[ind])][::-1]\n",
    "\n",
    "# Tokenizing using dictionary\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am so happy for you. ‚ò∫ üòä üíô üíõ ‚ù§\n"
     ]
    }
   ],
   "source": [
    "#@title Write the input text, and the model will recommend some emojis for you\n",
    "# examples/text_emojize.py\n",
    "\n",
    "input_text = 'I am so happy for you.' #@param \n",
    "maxlen = 30 #@param\n",
    "\n",
    "st = SentenceTokenizer(vocabulary, maxlen)\n",
    "\n",
    "# Loading model\n",
    "model = torchmoji_emojis(PRETRAINED_PATH)\n",
    "# Running predictions\n",
    "tokenized, _, _ = st.tokenize_sentences([input_text])\n",
    "\n",
    "# print(f\"tokenized: {tokenized}\")\n",
    "\n",
    "# Get sentence probability\n",
    "prob = model(tokenized)[0]\n",
    "\n",
    "# print(f\"prob: {prob}\")\n",
    "\n",
    "# Top emoji id\n",
    "emoji_ids = top_elements(prob, 5)\n",
    "\n",
    "# print(f\"emoji_ids: {emoji_ids}\")\n",
    "\n",
    "# map to emojis\n",
    "emojis = map(lambda x: EMOJIS[x], emoji_ids)\n",
    "\n",
    "print(emoji.emojize(\"{} {}\".format(input_text,' '.join(emojis)), use_aliases=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[0] I love mom's cooking:\t [-0.00852921  0.05861181  0.          0.          0.        ]\n",
      "Sentence[1] I love my parents:\t [-0.00868416  0.0420589   0.          0.          0.        ]\n",
      "Sentence[2] You are beautiful:\t [-0.02398385  0.02189129  0.          0.         -0.00982201]\n",
      "Sentence[3] You are so beautiful:\t [-0.0118494   0.01473236  0.          0.         -0.00892754]\n"
     ]
    }
   ],
   "source": [
    "#@title Text encoding\n",
    "# examples/encode_texts.py\n",
    "import json\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_feature_encoding\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "\n",
    "TEST_SENTENCES = ['I love mom\\'s cooking',\n",
    "                  'I love my parents',\n",
    "                  'You are beautiful',\n",
    "                  'You are so beautiful']\n",
    "\n",
    "maxlen = 30\n",
    "batch_size = 32\n",
    "\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "st = SentenceTokenizer(vocabulary, maxlen)\n",
    "tokenized, _, _ = st.tokenize_sentences(TEST_SENTENCES)\n",
    "\n",
    "# print('Loading model from {}.'.format(PRETRAINED_PATH))\n",
    "model = torchmoji_feature_encoding(PRETRAINED_PATH)\n",
    "# print(model)\n",
    "\n",
    "# print('Encoding texts..')\n",
    "encoding = model(tokenized)\n",
    "\n",
    "for i, sent in enumerate(TEST_SENTENCES):\n",
    "    print(f\"Sentence[{i}] {sent}:\\t {encoding[i, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('data/SS-Twitter/twitter_vocab.pickle', 'rb') as f:\n",
    "with open('data/PsychExp/combined_vocab.pickle', 'rb') as f:\n",
    "    x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'PsychExp',\n",
       " 'train_texts': array([[  63,   18, 2597, ...,    0,    0,    0],\n",
       "        [  63,   18,   58, ...,    0,    0,    0],\n",
       "        [  63,   18,  470, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [  63,   18,   25, ...,    0,    0,    0],\n",
       "        [  18,  837, 1779, ...,    0,    0,    0],\n",
       "        [ 629,   41,  135, ...,   84,    1,   11]], dtype=uint16),\n",
       " 'val_texts': array([[   63,    18,   627, ...,     0,     0,     0],\n",
       "        [   20,   273,    15, ...,     0,     0,     0],\n",
       "        [   18,   837,    15, ...,  9060, 26146,    16],\n",
       "        ...,\n",
       "        [   64,    15,   581, ...,    53,    23,   196],\n",
       "        [   63,    15,  3416, ...,     0,     0,     0],\n",
       "        [   63,    18,  3685, ...,    28,    10,   423]], dtype=uint16),\n",
       " 'test_texts': array([[  18,   25,   44, ...,   86, 2025,   62],\n",
       "        [  41,  520,   58, ..., 1252,   21,   10],\n",
       "        [  18, 1144,   41, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [4052,   36,   91, ...,    0,    0,    0],\n",
       "        [6607,    1,   41, ...,    0,    0,    0],\n",
       "        [  18,   25, 1465, ...,    0,    0,    0]], dtype=uint16),\n",
       " 'train_labels': array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'val_labels': array([[0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.]]),\n",
       " 'test_labels': array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('autoui2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18b1ba31593d0d231dc4ab171732a6e59282da54a145287df519e902286ff9cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
